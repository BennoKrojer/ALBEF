image_res: 384
batch_size: 8 # used to be 16 if you train with 8 GPUs, so now it is 8*16=128
grad_accumulation: 16

bert_config: 'configs/config_bert.json'

alpha: 0.4
distill: True
warm_up: True
eval_ema: False

optimizer: {opt: adamW, lr: 2e-5, weight_decay: 0.02}
schedular: {sched: cosine, lr: 2e-5, epochs: 10, min_lr: 1e-6, decay_rate: 1, warmup_lr: 1e-5, warmup_epochs: 1, cooldown_epochs: 0}







